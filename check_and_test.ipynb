{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2677e38d-b480-4d7d-a5ad-c25454e0d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc09485-0178-49e6-8b0a-d83db13602e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]\n",
      "environment: /home/aclexp/mambaforge/envs/quanta\n"
     ]
    }
   ],
   "source": [
    "print(\"version:\", sys.version)\n",
    "print(\"environment:\", sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7986992b-3399-4957-849d-f6fe12ca7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4f01e5-7680-4871-8ab4-2e772de8f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ca08ff-68f5-4a91-931f-017399b66f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a1dd7f-be83-4b12-a5bc-809dfbc07d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a8edb7-5063-4c71-93ae-ce399c42c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(os.path.join(\"rawdata\", \"DATA_ses-01_2024-12-09.csv\"))\n",
    "DF.rename(columns={\"BASIC_INFO_ID\": \"ID\"}, inplace=True)\n",
    "\n",
    "inclusion_df = pd.read_csv(os.path.join(\"rawdata\", \"InclusionList_ses-01.csv\"))\n",
    "inclusion_df = inclusion_df.query(\"MRI == 1\")\n",
    "\n",
    "DF = pd.merge(DF, inclusion_df[[\"ID\"]], on=\"ID\", how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401421e-c3d3-4d0c-b151-2b80912399ae",
   "metadata": {},
   "source": [
    "## Select features by specified domains and approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a1f633-65b5-4596-922f-e941ec10f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_features = [ \n",
    "    col for col in DF.columns\n",
    "    if any( domain in col for domain in [\"STRUCTURE\", \"MOTOR\", \"MEMORY\", \"LANGUAGE\"] )\n",
    "    and any( approach in col for approach in [\"MRI\", \"BEH\", \"EEG\"] )\n",
    "    and \"RESTING\" not in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274aa138-44ec-492d-99ac-0170ecce7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DF.loc[:, included_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d773540-a9d7-4e7f-9d0e-e4cd070d340c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af887eec-4150-4920-9f11-1b44147fa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_f = categorize_features(X.columns)\n",
    "\n",
    "# for f in sorted(cat_f.keys()):\n",
    "#     print(f\"# {f}: {len(cat_f[f])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c60a9-4c6c-4c26-b7d3-e5d397d0864b",
   "metadata": {},
   "source": [
    "## Remove features with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb5f1565-d398-4427-b415-ea93ae0c58e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 out of 1457 features have too many missing values (> 0.14).\n"
     ]
    }
   ],
   "source": [
    "n_subjs = len(X)\n",
    "na_rates = pd.Series(X.isnull().sum() / n_subjs)\n",
    "\n",
    "Q1 = na_rates.quantile(.25)\n",
    "Q3 = na_rates.quantile(.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = na_rates[na_rates > (Q3 + IQR * 1.5)]\n",
    "\n",
    "print(f\"{len(outliers)} out of {len(na_rates)} features have too many missing values (> {round(Q3 + IQR * 1.5, 2)}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982f6f6f-1d7c-4871-8379-ad4a50976bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = X.drop(columns=outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc0998e8-4c61-4937-a885-e3232106ea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1343"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b6cd7-3fa3-485b-8c24-0f30bcad39e0",
   "metadata": {},
   "source": [
    "## Fill missing values with median and standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d76953-88e7-4e92-8cf9-de000f523b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_cleaned), columns=X_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd14db87-4506-45f2-9c6c-dfb358cd9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X_imputed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4e3d7-6943-4017-b6a4-69a6a0e808b7",
   "metadata": {},
   "source": [
    "## Calculate VIF \n",
    "##### See: https://stats.stackexchange.com/questions/461141/is-it-advisable-to-impute-missing-values-and-scale-features-before-computing-the\n",
    "##### Also: https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "063c34a7-9204-4da6-908e-5ff138d9d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def variance_inflation_factor(X: np.ndarray, idx: int):\n",
    "#     mask = np.arange(X.shape[1]) != idx\n",
    "#     X_i = X[:, idx]\n",
    "#     X_else = X[:, mask]\n",
    "#     reg = LinearRegression(fit_intercept=True).fit(X_else, X_i)\n",
    "#     Xi_pred = reg.predict(X_else)\n",
    "#     R_sq = reg.score(X_else, X_i)\n",
    "#     # ss_tot = np.sum((X_i - np.mean(X_i)) ** 2)\n",
    "#     # ss_res = np.sum((X_i - Xi_pred) ** 2)\n",
    "#     # R_sq = 1 - (ss_res / ss_tot)\n",
    "#     vif = 1 / max(1 - R_sq, 1e-16)\n",
    "#     return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc15c265-2b0d-4ecf-a181-5422686d02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif_values = pd.Series(\n",
    "#     [ variance_inflation_factor(X_scaled.values, i) for i in range(X_scaled.shape[1]) ], \n",
    "#     index=X_scaled.columns, \n",
    "#     name='VIF'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc49094-0a93-48ae-8182-bfea006f12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406bfab8-416e-48ee-a34e-a86bf33e8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = add_constant(X_scaled)\n",
    "# vif_values = pd.Series(\n",
    "#     [ variance_inflation_factor(X_.values, i) for i in range(X_.shape[1]) ], \n",
    "#     index=X_.columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39cacde-2d66-48ad-be0f-7002e78f87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vifs = pd.Series(\n",
    "#     np.linalg.inv(X_scaled.corr().to_numpy()).diagonal(), # pseudo-inverse\n",
    "#     index=X_scaled.columns, \n",
    "#     name='VIF'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e759599-0f89-45de-bece-a2a8ee06bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vifs = vifs.sort_values(by=\"VIF\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9368f75-fee2-4d1a-844b-da6723d633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vifs[vifs.abs() < 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88769109-3164-4dac-a108-46373d36eb14",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9469e4ad-53be-43b1-9c72-0b1f2842ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CORRELATION\n",
      "count  1803649.000\n",
      "mean         0.028\n",
      "std          0.134\n",
      "min         -0.918\n",
      "25%         -0.043\n",
      "50%          0.008\n",
      "75%          0.066\n",
      "max          1.000\n"
     ]
    }
   ],
   "source": [
    "X_cormat = X_scaled.corr()\n",
    "X_corr = X_cormat.stack().reset_index()\n",
    "X_corr.columns = ['FEATURE_1', 'FEATURE_2', 'CORRELATION']\n",
    "print(X_corr.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c59de6d6-97fd-4264-98d3-d0cc3e6c7744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023247316966882136"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_corr['CORRELATION'] > .8).sum() / len(X_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6cfb1da-492a-4140-8971-9d13877c727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cormat.to_csv(os.path.join(\"derivatives\", \"Feature cormat.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f4a263-2670-4f45-a4c7-af578ef88863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(X_cormat, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502355f9-4e51-4364-b704-19657c26794b",
   "metadata": {},
   "source": [
    "## Categorize features based on knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6598ee90-9924-4e82-982f-a9c6f586ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_features(included_features):\n",
    "    categorized_features = {}\n",
    "    \n",
    "    for feature in included_features:\n",
    "        if feature.startswith(\"STRUCTURE\"):\n",
    "            category, hemi, area, measure = feature.split(\"_\")[3::]\n",
    "            if category == \"NULL\":\n",
    "                f = f\"STR ({category}, {measure})\"\n",
    "            else:\n",
    "                f = f\"STR ({category}, {hemi[0]}, {measure})\"\n",
    "            \n",
    "        else:\n",
    "            domain, task, measure, condition = feature.split(\"_\")[:4]\n",
    "            measure = \"fMRI\" if measure == \"MRI\" else measure                \n",
    "\n",
    "            if domain == \"LANGUAGE\":\n",
    "                f = f\"{measure} ({domain.lower()})\"\n",
    "                \n",
    "            elif (measure == \"EEG\") and (task == \"OSPAN\") and (\"Diff\" in condition):\n",
    "                f = f\"{measure} ({domain.lower()}, {task.lower()} diff)\"\n",
    "                \n",
    "            elif (measure == \"EEG\") and (task == \"GOFITTS\"):\n",
    "                suffix = re.sub(r\"[0-9]+\", \"\", condition).replace(\"ID\", \"\").replace(\"W\", \"\").replace(\"Slope\", \" slope\")\n",
    "                f = f\"{measure} ({domain.lower()}, {task.lower()} {suffix.lower()})\"\n",
    "            \n",
    "            else:\n",
    "                f = f\"{measure} ({domain.lower()}, {task.lower()})\"\n",
    "\n",
    "        if f in categorized_features.keys():\n",
    "            categorized_features[f].append(feature)\n",
    "        else:\n",
    "            categorized_features.update({f: [feature]})\n",
    "    \n",
    "    return categorized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b495e25-1c4d-4a4a-b3bc-48e913795f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_features = categorize_features(X_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba8b8540-8661-4e79-9a03-29beddfc36b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BEH (language): 14\n",
      "# BEH (memory, exclusion): 54\n",
      "# BEH (memory, mst): 33\n",
      "# BEH (memory, ospan): 20\n",
      "# BEH (motor, bilpress): 6\n",
      "# BEH (motor, gforce): 16\n",
      "# BEH (motor, gofitts): 24\n",
      "# EEG (memory, exclusion): 60\n",
      "# EEG (memory, ospan diff): 48\n",
      "# EEG (memory, ospan): 72\n",
      "# EEG (motor, bilpress): 32\n",
      "# EEG (motor, gofitts cue slope): 36\n",
      "# EEG (motor, gofitts cue): 72\n",
      "# EEG (motor, gofitts go slope): 36\n",
      "# EEG (motor, gofitts go): 72\n",
      "# STR (GM, L, FA): 34\n",
      "# STR (GM, L, ThickAvg): 74\n",
      "# STR (GM, L, ThickStd): 74\n",
      "# STR (GM, L, VOLUME): 74\n",
      "# STR (GM, R, FA): 34\n",
      "# STR (GM, R, ThickAvg): 74\n",
      "# STR (GM, R, ThickStd): 74\n",
      "# STR (GM, R, VOLUME): 74\n",
      "# STR (NULL, FA): 36\n",
      "# STR (WM, L, FA): 34\n",
      "# STR (WM, L, VOLUME): 34\n",
      "# STR (WM, R, FA): 34\n",
      "# STR (WM, R, VOLUME): 34\n",
      "# fMRI (motor, gforce): 64\n"
     ]
    }
   ],
   "source": [
    "for f in sorted(categorized_features.keys()):\n",
    "    print(f\"# {f}: {len(categorized_features[f])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47cfc51d-986a-4e65-a400-297c8197a6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1343"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ len(v) for v in categorized_features.values() ]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5956866c-1efd-4dc3-9ecd-e61b7d922935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BEH (memory, exclusion): 6\n",
      "# BEH (memory, mst): 2\n",
      "# STR (NULL, FA): 1\n",
      "# fMRI (memory, mst): 105\n"
     ]
    }
   ],
   "source": [
    "categorized_outliers = categorize_features(outliers.index)\n",
    "\n",
    "for f in sorted(categorized_outliers.keys()):\n",
    "    print(f\"# {f}: {len(categorized_outliers[f])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4ce2682-b1f6-4cae-b09a-f0018519669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_splited = {}\n",
    "for f in categorized_features.keys():\n",
    "    X_splited[f] = X_scaled.loc[:, categorized_features[f]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718664e7-1111-4640-91f1-9bacdf46065e",
   "metadata": {},
   "source": [
    "## Combining variables using PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37e9e8d9-d5e3-4f14-8fa0-2966a9188857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/62303782/is-there-a-way-to-conduct-a-parallel-analysis-in-python\n",
    "## https://www.statstodo.com/ParallelAnalysis.php\n",
    "\n",
    "def parallel_analysis(X, n_iter=100, seed=42):\n",
    "    n_components = X.shape[1] - 1\n",
    "    pca = PCA(n_components, random_state=seed)\n",
    "    X_transformed = pca.fit_transform(X)\n",
    "    eigenvalues = pca.explained_variance_ratio_\n",
    "\n",
    "    random_eigenvalues = np.zeros(shape=(n_iter, n_components))\n",
    "    for i in range(n_iter): # Monte Carlo simulation\n",
    "        X_r = np.random.normal(loc=0, scale=1, size=X.shape) # random data\n",
    "        pca_r = PCA(n_components, random_state=42)\n",
    "        X_r_transformed = pca_r.fit_transform(X_r)\n",
    "        random_eigenvalues[i, :] = pca_r.explained_variance_ratio_\n",
    "    \n",
    "    rand_eigv_mean = random_eigenvalues.mean(axis=0)\n",
    "    rand_eigv_std = random_eigenvalues.std(axis=0)\n",
    "    thresholds = rand_eigv_mean + rand_eigv_std * 1.64 # the 95th percentile\n",
    "    n_retained = max(np.argwhere(eigenvalues > thresholds)) + 1\n",
    "\n",
    "    return n_retained, eigenvalues, random_eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "137a55a5-522d-4eee-ae50-9a0666b1db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "n_retained_comps = {}\n",
    "\n",
    "for f, X_n in X_splited.items():\n",
    "    n_components, eigv_raw, eigv_rand = parallel_analysis(X_n)\n",
    "    n_retained_comps[f] = n_components[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ea49be0-1cd7-474c-8ba6-85ff539c5098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STR (GM, L, VOLUME)': 5,\n",
       " 'STR (GM, L, ThickAvg)': 5,\n",
       " 'STR (GM, L, ThickStd)': 4,\n",
       " 'STR (GM, R, VOLUME)': 5,\n",
       " 'STR (GM, R, ThickAvg)': 6,\n",
       " 'STR (GM, R, ThickStd)': 5,\n",
       " 'STR (WM, L, VOLUME)': 2,\n",
       " 'STR (WM, R, VOLUME)': 3,\n",
       " 'STR (NULL, FA)': 4,\n",
       " 'STR (GM, L, FA)': 5,\n",
       " 'STR (GM, R, FA)': 5,\n",
       " 'STR (WM, L, FA)': 2,\n",
       " 'STR (WM, R, FA)': 3,\n",
       " 'BEH (language)': 4,\n",
       " 'BEH (memory, exclusion)': 7,\n",
       " 'BEH (memory, ospan)': 3,\n",
       " 'BEH (memory, mst)': 6,\n",
       " 'EEG (memory, exclusion)': 11,\n",
       " 'EEG (memory, ospan)': 9,\n",
       " 'EEG (memory, ospan diff)': 8,\n",
       " 'BEH (motor, gforce)': 3,\n",
       " 'fMRI (motor, gforce)': 8,\n",
       " 'EEG (motor, gofitts cue slope)': 12,\n",
       " 'EEG (motor, gofitts go slope)': 9,\n",
       " 'EEG (motor, gofitts cue)': 7,\n",
       " 'EEG (motor, gofitts go)': 7,\n",
       " 'BEH (motor, gofitts)': 4,\n",
       " 'BEH (motor, bilpress)': 2,\n",
       " 'EEG (motor, bilpress)': 5}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_retained_comps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2a40b-26ec-40d3-b33e-fbcfac8f1625",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a8b2c-3075-4752-9bb8-15d4b4fffcac",
   "metadata": {},
   "source": [
    "##### Mundfrom, D. J., Shaw, D. G., & Ke, T. L. (2005). Minimum sample size recommendations for conducting factor analyses. https://doi.org/10.1207/s15327574ijt0502_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20695a2c-df2f-4cd3-8031-4e8c7b5d853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.6\n",
      "36.0\n",
      "48.6\n"
     ]
    }
   ],
   "source": [
    "sub_DF_list = [\n",
    "    DF[DF[\"BASIC_INFO_AGE\"].between(lb, ub)] for lb, ub in [(0, 44), (45, np.inf)]\n",
    "]\n",
    "\n",
    "print(len(DF) / 5)\n",
    "\n",
    "for sub_DF in sub_DF_list:\n",
    "    print(len(sub_DF) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64172093-46ff-493f-ad9f-29716bff94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n",
      "81.0\n"
     ]
    }
   ],
   "source": [
    "for sub_DF in sub_DF_list:\n",
    "    print(len(sub_DF) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eebee20b-b2b4-4c68-b640-967cc908e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([ \"_\".join(f.split(\"_\")[:3]) for f in DF.columns ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437078a-a84a-43f2-92df-60509a5b8b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
